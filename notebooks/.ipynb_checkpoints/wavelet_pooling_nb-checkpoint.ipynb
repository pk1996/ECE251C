{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pywt\n",
    "import pywt.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pywt.data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_(inp_img):\n",
    "    coeffs2 = pywt.dwt2(inp_img, 'db2')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    titles = ['Approximation', ' Horizontal detail',\n",
    "              'Vertical detail', 'Diagonal detail']\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    for i, a in enumerate([LL, LH, HL, HH]):\n",
    "        ax = fig.add_subplot(1, 4, i + 1)\n",
    "        ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "        ax.set_title(titles[i], fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return LL, LH, HL, HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80378f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LL = original\n",
    "level = 2\n",
    "print(LL.shape)\n",
    "for i in range(level):\n",
    "    LL, LH, HL, HH = dwt_(LL)\n",
    "    print(LL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = pywt.idwt2((LL, (LH, HL, HH)), 'db2')\n",
    "plt.imshow(img_f)\n",
    "print(img_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50458dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "coeffs2 = pywt.dwt2(original, 'db2')\n",
    "LL, (LH, HL, HH) = coeffs2\n",
    "# LL = np.vstack((np.hstack((LL,LH)), np.hstack((HL,HH))))\n",
    "# print(LL.shape)\n",
    "img_ = pywt.idwt2(([pywt.idwt2((LL, (LH, HL, HH)), 'db2')][0]  , (cv2.pyrUp(LH)[:512,:512], cv2.pyrUp(HL)[:512,:512], cv2.pyrUp(HH)[:512,:512])), 'db2')\n",
    "plt.imshow(img_)\n",
    "img_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa472b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptwt, pywt, torch\n",
    "import numpy as np\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ac73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pywt.wavelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfccfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.tensor(face)\n",
    "input_ = input_[:,:-1,:-1]\n",
    "input_ = torch.unsqueeze(torch.tensor(input_),  dim = 0).repeat(4,1,1,1)\n",
    "INPUT_SIZE_ = input_.size()\n",
    "INPUT_SIZE_CAHCED_ = INPUT_SIZE_\n",
    "print(INPUT_SIZE_)\n",
    "\n",
    "## -- forward -------------------------------------\n",
    "print('Forward funtion of wavelet pooling --')\n",
    "print('Input dim - ' + str(input_.shape))\n",
    "\n",
    "bs = INPUT_SIZE_[0]\n",
    "FORWARD_OUTPUT_ = []\n",
    "# loop over input as batching not supported\n",
    "for k in range(bs):\n",
    "    coefficients = ptwt.wavedec2(torch.squeeze(input_[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                level=2, mode=\"constant\")\n",
    "    \n",
    "    # 2nd order DWT\n",
    "    forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(\"haar\"))\n",
    "    FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "print(\"\")\n",
    "print(\"Output dim - \" + str(FORWARD_OUTPUT_.size()))\n",
    "print(\"----------------\")\n",
    "\n",
    "\n",
    "backward_input_ = FORWARD_OUTPUT_\n",
    "## -- backward -------------------------------------\n",
    "INPUT_SIZE_ = backward_input_.size()\n",
    "print(\"Input to backward - \" + str(INPUT_SIZE_))\n",
    "bs = INPUT_SIZE_[0]\n",
    "BACKWARD_OUTPUT_ = []\n",
    "upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "# loop over input as batching not supported\n",
    "for k in range(bs):\n",
    "    # 1st order DWT\n",
    "    coefficients = ptwt.wavedec2(torch.squeeze(backward_input_[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                    level=1, mode=\"constant\")\n",
    "    # upsample subbands\n",
    "    upsampled_subbands_ = []\n",
    "    upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "    upsampled_subbands_.append([])\n",
    "    for k in range(len(coefficients[1])):\n",
    "        upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "    upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "    \n",
    "    # IDWT\n",
    "    backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(\"haar\"))\n",
    "    BACKWARD_OUTPUT_.append(backward_output_.squeeze())\n",
    "\n",
    "BACKWARD_OUTPUT_ = torch.stack(BACKWARD_OUTPUT_)\n",
    "BACKWARD_OUTPUT_SHAPE_ = BACKWARD_OUTPUT_.shape\n",
    "\n",
    "if INPUT_SIZE_CAHCED_ != BACKWARD_OUTPUT_SHAPE_:\n",
    "    # Ensure output dim matches input dim\n",
    "    if INPUT_SIZE_CAHCED_[2] != BACKWARD_OUTPUT_SHAPE_[2]:\n",
    "        BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:-1,:]\n",
    "    if INPUT_SIZE_CAHCED_[3] != BACKWARD_OUTPUT_SHAPE_[3]:\n",
    "        BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:,:-1]\n",
    "     \n",
    "\n",
    "print(\"Output of backward - \" + str(BACKWARD_OUTPUT_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132da23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([(1,2,3,4), \"aa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5210c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import ptwt, pywt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0fd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletPooling(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        INPUT_SIZE_ = x.size()\n",
    "        ctx.save_for_backward(torch.tensor(INPUT_SIZE_))\n",
    "        \n",
    "        bs = INPUT_SIZE_[0]\n",
    "        FORWARD_OUTPUT_ = []\n",
    "        \n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                        level=2, mode=\"constant\")\n",
    "\n",
    "            # 2nd order DWT\n",
    "            forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(\"haar\"))\n",
    "            FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "        FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "        \n",
    "        return FORWARD_OUTPUT_\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, x):\n",
    "        '''\n",
    "        Applies IDWT on upsampled 1st level DWT of the \n",
    "        input.\n",
    "        '''\n",
    "        INPUT_CAHCED_, = ctx.saved_tensors\n",
    "                            \n",
    "        bs = x.size()[0]\n",
    "\n",
    "        BACKWARD_OUTPUT_ = []\n",
    "        upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            # 1st order DWT\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                            level=1, mode=\"constant\")\n",
    "            # upsample subbands\n",
    "            upsampled_subbands_ = []\n",
    "            upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , \\\n",
    "                                       torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "            upsampled_subbands_.append([])\n",
    "            for k in range(len(coefficients[1])):\n",
    "                upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "            upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "\n",
    "            # IDWT\n",
    "            backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(\"haar\"))\n",
    "            BACKWARD_OUTPUT_.append(backward_output_.squeeze())\n",
    "\n",
    "        BACKWARD_OUTPUT_ = torch.stack(BACKWARD_OUTPUT_)\n",
    "        BACKWARD_OUTPUT_SHAPE_ = BACKWARD_OUTPUT_.shape\n",
    "        INPUT_SIZE_CAHCED_, = ctx.saved_tensors\n",
    "        INPUT_SIZE_CAHCED_ = tuple(INPUT_SIZE_CAHCED_.tolist())\n",
    "\n",
    "        if INPUT_SIZE_CAHCED_ != BACKWARD_OUTPUT_SHAPE_:\n",
    "            # Ensure output dim matches input dim\n",
    "            if INPUT_SIZE_CAHCED_[2] != BACKWARD_OUTPUT_SHAPE_[2]:\n",
    "                BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:-1,:]\n",
    "            if INPUT_SIZE_CAHCED_[3] != BACKWARD_OUTPUT_SHAPE_[3]:\n",
    "                BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:,:-1]\n",
    "        \n",
    "        return BACKWARD_OUTPUT_, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14162e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "input_.requires_grad = True\n",
    "\n",
    "layer = WaveletPooling.apply\n",
    "\n",
    "# forward\n",
    "a = torch.full((), 0.0, dtype=torch.float, requires_grad=True)\n",
    "output_ = a + layer(input_)\n",
    "\n",
    "loss = (output_ - torch.zeros_like(output_)).pow(2).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a20af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import ptwt, pywt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e4ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletPooling(nn.Module):\n",
    "    def __init__(self, wavelet = \"haar\"):\n",
    "        super(WaveletPooling, self).__init__()\n",
    "        self.wavelet = wavelet\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        bs = x.size()[0]\n",
    "        FORWARD_OUTPUT_ = []\n",
    "        \n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(self.wavelet),\n",
    "                                        level=2, mode=\"constant\")\n",
    "\n",
    "            # 2nd order DWT\n",
    "            forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(self.wavelet))\n",
    "            FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "        FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "        \n",
    "        return FORWARD_OUTPUT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04477bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_hook(module, input_, output_):\n",
    "    print(\"here\")\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4841345a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7faa7c05ec40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveletPooling_layer = WaveletPooling()\n",
    "≈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3b5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "input_.requires_grad = True\n",
    "\n",
    "layer = WaveletPooling()\n",
    "\n",
    "# forward\n",
    "a = torch.full((), 0.0, dtype=torch.float, requires_grad=True)\n",
    "output_ = layer(input_) + a\n",
    "\n",
    "loss = (output_ - torch.zeros_like(output_)).pow(2).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad00d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
