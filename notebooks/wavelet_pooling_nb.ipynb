{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388c17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pywt\n",
    "import pywt.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pywt.data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_(inp_img):\n",
    "    coeffs2 = pywt.dwt2(inp_img, 'db2')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    titles = ['Approximation', ' Horizontal detail',\n",
    "              'Vertical detail', 'Diagonal detail']\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    for i, a in enumerate([LL, LH, HL, HH]):\n",
    "        ax = fig.add_subplot(1, 4, i + 1)\n",
    "        ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "        ax.set_title(titles[i], fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return LL, LH, HL, HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80378f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LL = original\n",
    "level = 2\n",
    "print(LL.shape)\n",
    "for i in range(level):\n",
    "    LL, LH, HL, HH = dwt_(LL)\n",
    "    print(LL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = pywt.idwt2((LL, (LH, HL, HH)), 'db2')\n",
    "plt.imshow(img_f)\n",
    "print(img_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50458dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "coeffs2 = pywt.dwt2(original, 'db2')\n",
    "LL, (LH, HL, HH) = coeffs2\n",
    "# LL = np.vstack((np.hstack((LL,LH)), np.hstack((HL,HH))))\n",
    "# print(LL.shape)\n",
    "img_ = pywt.idwt2(([pywt.idwt2((LL, (LH, HL, HH)), 'db2')][0]  , (cv2.pyrUp(LH)[:512,:512], cv2.pyrUp(HL)[:512,:512], cv2.pyrUp(HH)[:512,:512])), 'db2')\n",
    "plt.imshow(img_)\n",
    "img_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd172bc",
   "metadata": {},
   "source": [
    "Replicating forward and backward passes for wavelet pooling using pywt, ptwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa472b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptwt, pywt, torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfccfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 16, 16])\n",
      "Forward funtion of wavelet pooling --\n",
      "Input dim - torch.Size([4, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# face = np.transpose(scipy.misc.face(),\n",
    "#                         [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.ones(3,16,16) #torch.tensor(face)\n",
    "# input_ = input_[:,:-1,:-1]\n",
    "input_ = torch.unsqueeze(torch.tensor(input_),  dim = 0).repeat(4,1,1,1)\n",
    "INPUT_SIZE_ = input_.size()\n",
    "INPUT_SIZE_CAHCED_ = INPUT_SIZE_\n",
    "print(INPUT_SIZE_)\n",
    "\n",
    "## -- forward -------------------------------------\n",
    "print('Forward funtion of wavelet pooling --')\n",
    "print('Input dim - ' + str(input_.shape))\n",
    "\n",
    "bs = INPUT_SIZE_[0]\n",
    "FORWARD_OUTPUT_ = []\n",
    "# loop over input as batching not supported\n",
    "for k in range(bs):\n",
    "    coefficients = ptwt.wavedec2(torch.squeeze(input_[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                level=2, mode=\"constant\")\n",
    "    \n",
    "    # 2nd order DWT\n",
    "    forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(\"haar\"))\n",
    "    FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "# print(FORWARD_OUTPUT_)\n",
    "# print(\"\")\n",
    "# print(\"Output dim - \" + str(FORWARD_OUTPUT_.size()))\n",
    "# print(\"----------------\")\n",
    "\n",
    "\n",
    "# backward_input_ = FORWARD_OUTPUT_\n",
    "# ## -- backward -------------------------------------\n",
    "# INPUT_SIZE_ = backward_input_.size()\n",
    "# print(\"Input to backward - \" + str(INPUT_SIZE_))\n",
    "# bs = INPUT_SIZE_[0]\n",
    "# BACKWARD_OUTPUT_ = []\n",
    "# upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "# # loop over input as batching not supported\n",
    "# for k in range(bs):\n",
    "#     # 1st order DWT\n",
    "#     coefficients = ptwt.wavedec2(torch.squeeze(backward_input_[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "#                                     level=1, mode=\"constant\")\n",
    "#     # upsample subbands\n",
    "#     upsampled_subbands_ = []\n",
    "#     upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "#     upsampled_subbands_.append([])\n",
    "#     for k in range(len(coefficients[1])):\n",
    "#         upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "#     upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "    \n",
    "#     # IDWT\n",
    "#     backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(\"haar\"))\n",
    "#     BACKWARD_OUTPUT_.append(backward_output_.squeeze())\n",
    "\n",
    "# BACKWARD_OUTPUT_ = torch.stack(BACKWARD_OUTPUT_)\n",
    "# BACKWARD_OUTPUT_SHAPE_ = BACKWARD_OUTPUT_.shape\n",
    "\n",
    "# if INPUT_SIZE_CAHCED_ != BACKWARD_OUTPUT_SHAPE_:\n",
    "#     # Ensure output dim matches input dim\n",
    "#     if INPUT_SIZE_CAHCED_[2] != BACKWARD_OUTPUT_SHAPE_[2]:\n",
    "#         BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:-1,:]\n",
    "#     if INPUT_SIZE_CAHCED_[3] != BACKWARD_OUTPUT_SHAPE_[3]:\n",
    "#         BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:,:-1]\n",
    "     \n",
    "\n",
    "# print(\"Output of backward - \" + str(BACKWARD_OUTPUT_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients[0], coefficients[1][0], coefficients[1][1], coefficients[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de9dfb",
   "metadata": {},
   "source": [
    "Wavelet pooling layer as a torch autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b865790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import ptwt, pywt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletPooling_(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, wavelet = \"haar\"):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        print('WAVELET POOLING FORWARD....')\n",
    "        INPUT_SIZE_ = x.size()\n",
    "        ctx.save_for_backward(torch.tensor(INPUT_SIZE_))\n",
    "        ctx.wavelet = wavelet\n",
    "        \n",
    "        bs = INPUT_SIZE_[0]\n",
    "        FORWARD_OUTPUT_ = []\n",
    "        \n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(wavelet),\n",
    "                                        level=2, mode=\"constant\")\n",
    "\n",
    "            # 2nd order DWT\n",
    "            forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(wavelet))\n",
    "            FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "        FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "        \n",
    "        return FORWARD_OUTPUT_\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, x):\n",
    "        '''\n",
    "        Applies IDWT on upsampled 1st level DWT of the \n",
    "        input.\n",
    "        '''\n",
    "        print('WAVELET POOLING BACKWARD....')\n",
    "        INPUT_CAHCED_, = ctx.saved_tensors\n",
    "        print(ctx.wavelet)\n",
    "                            \n",
    "        bs = x.size()[0]\n",
    "\n",
    "        BACKWARD_OUTPUT_ = []\n",
    "        upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            # 1st order DWT\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(\"haar\"),\n",
    "                                            level=1, mode=\"constant\")\n",
    "            # upsample subbands\n",
    "            upsampled_subbands_ = []\n",
    "            upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , \\\n",
    "                                       torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "            upsampled_subbands_.append([])\n",
    "            for k in range(len(coefficients[1])):\n",
    "                upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "            upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "\n",
    "            # IDWT\n",
    "            backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(\"haar\"))\n",
    "            BACKWARD_OUTPUT_.append(backward_output_.squeeze())\n",
    "\n",
    "        BACKWARD_OUTPUT_ = torch.stack(BACKWARD_OUTPUT_)\n",
    "        BACKWARD_OUTPUT_SHAPE_ = BACKWARD_OUTPUT_.shape\n",
    "        INPUT_SIZE_CAHCED_, = ctx.saved_tensors\n",
    "        INPUT_SIZE_CAHCED_ = tuple(INPUT_SIZE_CAHCED_.tolist())\n",
    "\n",
    "        # Ensure output dim matches input dim\n",
    "        if INPUT_SIZE_CAHCED_ != BACKWARD_OUTPUT_SHAPE_:\n",
    "            if INPUT_SIZE_CAHCED_[2] != BACKWARD_OUTPUT_SHAPE_[2]:\n",
    "                BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:-1,:]\n",
    "            if INPUT_SIZE_CAHCED_[3] != BACKWARD_OUTPUT_SHAPE_[3]:\n",
    "                BACKWARD_OUTPUT_ = BACKWARD_OUTPUT_[:,:,:,:-1]\n",
    "        \n",
    "        return BACKWARD_OUTPUT_, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing wavelet pooling autograd function\n",
    "# face = np.transpose(scipy.misc.face(),\n",
    "#                         [2, 0, 1]).astype(np.float64)\n",
    "# input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "# input_.requires_grad = True\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# y = WaveletPooling_.apply(input_)\n",
    "# target_ = torch.zeros_like(y)\n",
    "# loss = criterion(output_, target_)\n",
    "# loss.requires_grad = True\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,3,1,1)\n",
    "        self.pool = WaveletPooling_.apply\n",
    "        self.conv2 = nn.Conv2d(3,1,1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        y = self.conv1(x)\n",
    "        y = self.pool(x)\n",
    "#         y = self.conv2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14162e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "m = Model()\n",
    "\n",
    "optimizer = optim.SGD(m.parameters(), lr=1e-3)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "output_ = m(input_.float())\n",
    "target_ = torch.zeros_like(output_)\n",
    "print(output_.shape, target_.shape)\n",
    "\n",
    "loss = criterion(output_, target_)\n",
    "print(loss.requires_grad)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4972b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb2ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e96b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79065a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import ptwt, pywt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c002fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletPooling(nn.Module):\n",
    "    def __init__(self, wavelet = \"haar\"):\n",
    "        super(WaveletPooling, self).__init__()\n",
    "        self.wavelet = wavelet\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        bs = x.size()[0]\n",
    "        FORWARD_OUTPUT_ = []\n",
    "        \n",
    "        # loop over input as batching not supported\n",
    "        for k in range(bs):\n",
    "            coefficients = ptwt.wavedec2(torch.squeeze(x[k,:,:,:]), pywt.Wavelet(self.wavelet),\n",
    "                                        level=2, mode=\"constant\")\n",
    "\n",
    "            # 2nd order DWT\n",
    "            forward_output_ = ptwt.waverec2([coefficients[0], coefficients[1]], pywt.Wavelet(self.wavelet))\n",
    "            FORWARD_OUTPUT_.append(torch.squeeze(forward_output_, dim = 1))\n",
    "\n",
    "        FORWARD_OUTPUT_ = torch.stack(FORWARD_OUTPUT_)\n",
    "        print('Forward ...')\n",
    "        print(x.size(), FORWARD_OUTPUT_.size())\n",
    "        return FORWARD_OUTPUT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a03c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_hook(self, input_, output_):\n",
    "    print('Hook...')\n",
    "    print(input_[0].shape, output_[0].shape)\n",
    "    upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "    gradient = []\n",
    "    # loop over input as batching not supported\n",
    "    for k in range(len(input_)):\n",
    "        # 1st order DWT\n",
    "        coefficients = ptwt.wavedec2(input_[k], pywt.Wavelet(self.wavelet),\n",
    "                                        level=1, mode=\"constant\")\n",
    "        # upsample subbands\n",
    "        upsampled_subbands_ = []\n",
    "        upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , \\\n",
    "                                   torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "        upsampled_subbands_.append([])\n",
    "        for k in range(len(coefficients[1])):\n",
    "            upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "        upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "\n",
    "        # IDWT\n",
    "        backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(self.wavelet))\n",
    "        gradient.append(backward_output_.squeeze())\n",
    "    input_ = tuple(gradient)\n",
    "    print(input_[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78554b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "input_.requires_grad = True\n",
    "\n",
    "layer = WaveletPooling()\n",
    "layer.register_backward_hook(gradient_hook)\n",
    "optimizer = optim.SGD(layer.parameters(), lr=1e-3)\n",
    "\n",
    "# forward\n",
    "layer.train()\n",
    "# a = torch.full((), 0.0, dtype=torch.float, requires_grad=True)\n",
    "output_ = layer(input_) #+ a\n",
    "\n",
    "loss = (output_ - torch.zeros_like(output_)).pow(2).sum()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931aa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooling, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Dowsamples by a factor of 2 by applying IDWT on the \n",
    "        second level subband of the DWT output.\n",
    "        '''\n",
    "        return self.pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_hook(self, input_, output_):\n",
    "    print(input_[0].shape, output_[0].shape)\n",
    "#     upsample_ = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#     gradient = []\n",
    "#     # loop over input as batching not supported\n",
    "#     for k in range(len(input_)):\n",
    "#         # 1st order DWT\n",
    "#         coefficients = ptwt.wavedec2(input_[k], pywt.Wavelet(self.wavelet),\n",
    "#                                         level=1, mode=\"constant\")\n",
    "#         # upsample subbands\n",
    "#         upsampled_subbands_ = []\n",
    "#         upsampled_subbands_.append(torch.cat(( torch.cat((coefficients[0], coefficients[1][0]), dim = 3) , \\\n",
    "#                                    torch.cat((coefficients[1][1], coefficients[1][2]), dim = 3)), dim = 2))\n",
    "#         upsampled_subbands_.append([])\n",
    "#         for k in range(len(coefficients[1])):\n",
    "#             upsampled_subbands_[-1].append(upsample_(coefficients[1][k]))\n",
    "#         upsampled_subbands_[-1] = tuple(upsampled_subbands_[-1])  \n",
    "\n",
    "#         # IDWT\n",
    "#         backward_output_ = ptwt.waverec2(upsampled_subbands_, pywt.Wavelet(self.wavelet))\n",
    "#         gradient.append(backward_output_.squeeze())\n",
    "#     input_ = tuple(gradient)\n",
    "#     print(input_[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00912b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.transpose(scipy.misc.face(),\n",
    "                        [2, 0, 1]).astype(np.float64)\n",
    "input_ = torch.unsqueeze(torch.tensor(face),  dim = 0).repeat(4,1,1,1)\n",
    "input_.requires_grad = True\n",
    "\n",
    "layer = Pooling()\n",
    "layer.register_full_backward_hook(gradient_hook)\n",
    "\n",
    "# forward\n",
    "layer.train()\n",
    "# a = torch.full((), 0.0, dtype=torch.float, requires_grad=True)\n",
    "output_ = layer(input_) #+ a\n",
    "print(input_.size(), output_.size())\n",
    "\n",
    "loss = (output_ - torch.zeros_like(output_)).pow(2).sum()\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
